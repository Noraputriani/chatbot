{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset untuk training dihasilkan dengan menggunakan GPT-4 dari API OpenAI\n",
        "Saya menggunakan API GPT-4 dari OpenAI untuk membuat dataset dengan informasi yang lebih akurat tanpa adanya halusinasi AI. Penggunaan GPT-4 memungkinkan saya untuk menghasilkan data yang lebih terpercaya dan dapat diandalkan dalam skala besar.\n",
        "\n",
        "### Estimasi Biaya untuk 100 Ribuan Baris Data\n",
        "Dengan menggunakan GPT-4 untuk menghasilkan dataset sebanyak 100 ribu baris data, estimasi biayanya berkisar antara $10 hingga $25. Biaya tersebut bergantung pada model spesifik yang digunakan, frekuensi penggunaan, dan perincian tarif dari API OpenAI.\n",
        "\n",
        "### Alasan Tidak Melakukan Web Scraping\n",
        "Saya memilih untuk tidak melakukan web scraping untuk mendapatkan dataset karena ingin menghindari pelanggaran terhadap Term of Service (ToS) atau aturan penggunaan dari situs web yang di-scrape. Melanggar ToS bisa berakibat pada sanksi hukum, oleh karena itu saya memilih untuk menggunakan API GPT-4 dari OpenAI sebagai alternatif yang sah dan legal untuk menghasilkan dataset yang dibutuhkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import winsound\n",
        "\n",
        "openai.api_key = 'sk-pzEuO5Dv7Yxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
        "\n",
        "def play_error_sound():\n",
        "    try:\n",
        "        winsound.PlaySound(\"SystemExclamation\", winsound.SND_ALIAS)\n",
        "    except Exception as e:\n",
        "        print(f\"Error playing sound: {e}\")\n",
        "\n",
        "\n",
        "for i in range(0, 1000):\n",
        "    print(i + \"Request berhasil\")\n",
        "\n",
        "    try:\n",
        "        respons = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": f\"\"\"Buat pertanyaan seputar kesehatan sebanyak 100 pertanyaan beserta jawaban. berikan informasi yang valid dan fakta. selalu sarankan untuk pergi kedokter jika keadaan atau keluhan serius.\n",
        "                 output yang di harapkan adalah sebagai berikut :\n",
        "                 pertanyaan :\n",
        "                 jawaban :\n",
        "                 pertanyaan :\n",
        "                 jawaban :\n",
        "                 \"\"\"}\n",
        "            ])\n",
        "\n",
        "        content_only = respons['choices'][0]['message']['content']\n",
        "        with open('output.txt', 'a', encoding='utf-8') as output_file:\n",
        "            output_file.write(content_only + \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        play_error_sound()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lakukan data cleaning, dengan menghapus output yang tidak di harapkan, lakukan pengecekan secara manual, dikarenakan data yang dihasilkan pasti akan berbeda beda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file_path = 'output.txt'\n",
        "output_file_path = 'dataset.txt'\n",
        "\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "filtered_lines = []\n",
        "\n",
        "for i in range(len(lines) - 1):\n",
        "    if lines[i].strip() == 'pertanyaan :' and lines[i + 1].strip() == 'jawaban :':\n",
        "        filtered_lines.extend([lines[i], lines[i + 1]])\n",
        "\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    output_file.writelines(filtered_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ambil sampel sekitar 10% dari data untuk di evaluasi fakta dan kebenarannya secara manual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "file_path = 'dataset.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    all_lines = file.readlines()\n",
        "\n",
        "total_lines = len(all_lines)\n",
        "num_random_lines = 2000\n",
        "\n",
        "if total_lines <= num_random_lines:\n",
        "    selected_lines = all_lines\n",
        "else:\n",
        "    random_indexes = random.sample(range(total_lines), num_random_lines)\n",
        "    selected_lines = [all_lines[i] for i in random_indexes]\n",
        "\n",
        "for line in selected_lines:\n",
        "    print(line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open(\"dataset.txt\", \"r\") as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "test_size = int(len(data) * 0.1)\n",
        "test_start_idx = (len(data) - test_size) // 2\n",
        "test_end_idx = test_start_idx + test_size\n",
        "\n",
        "train = data[:test_start_idx] + data[test_end_idx:]\n",
        "test = data[test_start_idx:test_end_idx]\n",
        "\n",
        "with open(\"train_dataset.txt\", \"w\") as file:\n",
        "    file.writelines(train)\n",
        "\n",
        "with open(\"test_dataset.txt\", \"w\") as file:\n",
        "    file.writelines(test)\n",
        "\n",
        "print(\"Training set size:\", len(train))\n",
        "print(\"Testing set size:\", len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inisialisasi tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indonesian-nlp/gpt2-medium-indonesian\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=64)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=64)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inisiasi Parameter Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"indonesian-nlp/gpt2-medium-indonesian\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetune\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=64,\n",
        "    eval_steps=400,\n",
        "    save_steps=800,\n",
        "    warmup_steps=500,\n",
        "    prediction_loss_only=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100, \n",
        "    logging_first_step=False,\n",
        "    save_total_limit=5,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training dan save model\n",
        "Training model membutuhkan GPU dengan vram minimal 16gb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "KFoiU2F_SJ6p",
        "outputId": "4857a207-2961-4c48-cf79-0ea67b78c8a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9813' max='9813' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9813/9813 4:26:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.948100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.704900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.628100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.548900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>2.469500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.406400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>2.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.387600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>2.387500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>2.379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>2.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>2.301400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>2.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>2.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>2.294000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>2.288700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>2.284100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9813, training_loss=2.4421457997960294, metrics={'train_runtime': 15987.9917, 'train_samples_per_second': 19.637, 'train_steps_per_second': 0.614, 'total_flos': 3.64456978808832e+16, 'train_loss': 2.4421457997960294, 'epoch': 3.0})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gR5ATIXkSN2S"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "doc_gpt_train",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
